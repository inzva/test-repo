{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# both keras and tf datasets can be used. TFDS will be used to show the list of datasets.\n",
    "# TFDS is a high-level wrapper around tf.data.\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of datasets\n",
    "tfds.list_builders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary libraries for data exploration and further data operations\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use tfds api to get mnist dataset\n",
    "# split to train and test\n",
    "# batch size -1, thus, no batch yet.\n",
    "mnist_training, mnist_test = tfds.load('mnist', split=['train', 'test'], batch_size=-1, as_supervised=True)                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mnist_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mnist_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_training_images, mnist_training_labels = mnist_training[0], mnist_training[1]\n",
    "mnist_test_images, mnist_test_labels = mnist_test[0], mnist_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mnist_training_images.shape)\n",
    "print(mnist_training_labels.shape)\n",
    "\n",
    "print(mnist_test_images.shape)\n",
    "print(mnist_test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# let's visualize first training image\n",
    "plt.imshow(mnist_training_images[0] ,cmap = 'gray')\n",
    "print(mnist_training_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# also, let's visualize first test image\n",
    "plt.imshow(mnist_test_images[0] ,cmap = 'gray')\n",
    "print(mnist_test_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "# reshape into trainable vectors\n",
    "num_training_images = mnist_training_images.shape[0]\n",
    "num_test_images = mnist_test_images.shape[0]\n",
    "\n",
    "img_width, img_height = mnist_training_images.shape[1], mnist_training_images.shape[2]\n",
    "\n",
    "# since dense layer, we have to flatten 28x28 to 784x1.\n",
    "mnist_training_images = tf.reshape(mnist_training_images, shape=(num_training_images, img_width * img_height))\n",
    "mnist_test_images = tf.reshape(mnist_test_images, shape=(num_test_images, img_width * img_height))\n",
    "\n",
    "# check the changes\n",
    "print(mnist_training_images.shape)\n",
    "print(mnist_test_images.shape)\n",
    "\n",
    "# another preprocessing step is to normalize data\n",
    "print(np.amax(mnist_training_images[0]),np.amin(mnist_training_images[0]))\n",
    "\n",
    "print(np.amax(mnist_test_images[0]),np.amin(mnist_test_images[0]))\n",
    "\n",
    "print(np.amax(mnist_training_labels),np.amin(mnist_training_labels))\n",
    "\n",
    "print(np.amax(mnist_test_labels),np.amin(mnist_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversion of data type and normalization of training data\n",
    "# main idea of normalization/standardization -> variables that are at different scale contribute different.\n",
    "# we want to reduce the \"bias\" as much as possible by these methods.\n",
    "# min-max is highly influenced by outliers! min and max values affect a lot!\n",
    "def preprocess(x, y):\n",
    "  x = tf.cast(x, tf.float32) / 255.0\n",
    "  y = tf.cast(y, tf.int64)\n",
    "\n",
    "  return x, y\n",
    "\n",
    "# one-hot labels and create dataloader with given batch size.\n",
    "def create_dataset(xs, ys, n_classes=10):\n",
    "  ys = tf.one_hot(ys, depth=n_classes)\n",
    "  return tf.data.Dataset.from_tensor_slices((xs, ys)) \\\n",
    "    .map(preprocess) \\\n",
    "    .shuffle(len(ys)) \\\n",
    "    .batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = create_dataset(mnist_training_images, mnist_training_labels)\n",
    "test_dataset = create_dataset(mnist_test_images, mnist_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset)\n",
    "train_dataset.element_spec    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the dataloader\n",
    "batch_images, batch_labels = next(iter(train_dataset))\n",
    "print(batch_images.shape)\n",
    "print(batch_labels.shape)\n",
    "print(batch_images[0])\n",
    "print(batch_labels[0])\n",
    "print(np.amax(batch_images[0]),np.amin(batch_images[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize first batch training image to show it is corresponds to same class with printed label.\n",
    "plt.imshow(tf.reshape(batch_images[0], shape=(img_width, img_height, 1)) ,cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining hyperparameters\n",
    "input_shape = 784\n",
    "label_shape = 10\n",
    "\n",
    "lr = 0.003\n",
    "\n",
    "layer_neurons = [\n",
    "    [input_shape, 200],\n",
    "    [200, 80],\n",
    "    [80, label_shape],\n",
    "]\n",
    "\n",
    "bias_shapes = [200, 80, label_shape]\n",
    "# xaiver uniform initializer\n",
    "initializer = tf.initializers.glorot_uniform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a dense layer, also, you can use TF2 API or Keras!\n",
    "def dense_layer(inputs, weights, bias):\n",
    "    return tf.nn.sigmoid(tf.matmul(inputs, weights) + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for initialization of weights and biases\n",
    "def get_weight(shape, name):\n",
    "    return tf.Variable(initializer(shape), name=name, trainable=True, dtype=tf.float32)\n",
    "\n",
    "def get_bias(shape, name):\n",
    "    return tf.Variable(initializer([shape]), name=name, trainable=True, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define weights and bias lists to use in model\n",
    "weights = []\n",
    "bias = []\n",
    "i = 0\n",
    "for layer in layer_neurons:\n",
    "    weights.append(get_weight(layer, 'weight{}'.format(i)))\n",
    "    i+=1\n",
    "\n",
    "i = 0\n",
    "for layer in bias_shapes:\n",
    "    bias.append(get_bias(layer, 'bias{}'.format(i)))\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model with initialized weights and biases\n",
    "def model(input):\n",
    "    l1 = dense_layer(input, weights[0], bias[0])\n",
    "    l2 = dense_layer(l1, weights[1], bias[1])\n",
    "    l3 = dense_layer(l2, weights[2], bias[2])\n",
    "    #return tf.nn.softmax(l3)\n",
    "    return l3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define optimizer and loss function\n",
    "optimizer = tf.optimizers.Adam(lr)\n",
    "\n",
    "# it is with logits because we return the predictions without applying softmax!\n",
    "# applied directly to prediction probabilities.\n",
    "def loss(pred, target):\n",
    "    return tf.nn.softmax_cross_entropy_with_logits(target, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define our train_step here\n",
    "# tf.GradientTape is used for recording operations for automatic differentiation. backward pass!\n",
    "def train_step(model, inputs, outputs, epoch):\n",
    "    epoch_loss_avg = None\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        current_loss = loss(model(inputs), outputs)\n",
    "        grads = tape.gradient(current_loss, weights)\n",
    "        optimizer.apply_gradients(zip(grads, weights))\n",
    "    \n",
    "    epoch_loss_avg = tf.reduce_mean(current_loss)\n",
    "    \n",
    "    return epoch_loss_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "# batch by batch for each epoch -> traverse over all training dataset.\n",
    "# total loss is divided by number of iterations to get average loss for each batch.\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    i = 0\n",
    "    for train_data in train_dataset:\n",
    "        batch_images, batch_labels = train_data\n",
    "        iter_loss = train_step(model, batch_images, batch_labels, epoch)\n",
    "        epoch_loss += iter_loss\n",
    "        i+=1\n",
    "    print(\"--- On epoch {} ---\".format(epoch))\n",
    "    tf.print(\"| Loss: \", epoch_loss/i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = 0 \n",
    "# use trained model over test dataset and normalize with number of test samples\n",
    "# obtain accuracy!\n",
    "for test_data in test_dataset:\n",
    "    batch_images, batch_labels = test_data\n",
    "    predictions = model(batch_images)\n",
    "    predictions = tf.nn.softmax(predictions)\n",
    "    equality = tf.math.equal(np.argmax(predictions, axis=1), np.argmax(batch_labels, axis=1))\n",
    "    acc += np.sum(equality)\n",
    "acc /= 10000\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
