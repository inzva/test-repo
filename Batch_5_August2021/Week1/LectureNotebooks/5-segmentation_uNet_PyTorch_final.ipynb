{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "from datasets.segmentation_dataset import SegmentationData, label_img_to_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = os.path.join('./datasets','segmentation')\n",
    "\n",
    "train_data = SegmentationData(image_paths_file=f'{data_root}/segmentation_data/train.txt')\n",
    "val_data = SegmentationData(image_paths_file=f'{data_root}/segmentation_data/val.txt')\n",
    "test_data = SegmentationData(image_paths_file=f'{data_root}/segmentation_data/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train size: %i\" % len(train_data))\n",
    "print(\"Validation size: %i\" % len(val_data))\n",
    "print(\"Img size: \", train_data[0][0].size())\n",
    "print(\"Segmentation size: \", train_data[0][1].size())\n",
    "\n",
    "num_example_imgs = 4\n",
    "plt.figure(figsize=(10, 5 * num_example_imgs))\n",
    "for i, (img, target) in enumerate(train_data[:num_example_imgs]):\n",
    "    # img\n",
    "    plt.subplot(num_example_imgs, 2, i * 2 + 1)\n",
    "    plt.imshow(img.numpy().transpose(1,2,0))\n",
    "    plt.axis('off')\n",
    "    if i == 0:\n",
    "        plt.title(\"Input image\")\n",
    "    \n",
    "    # target\n",
    "    plt.subplot(num_example_imgs, 2, i * 2 + 2)\n",
    "    plt.imshow(label_img_to_rgb(target.numpy()))\n",
    "    plt.axis('off')\n",
    "    if i == 0:\n",
    "        plt.title(\"Target image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class SegmentationNN(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=23, hparams=None):\n",
    "        super().__init__()\n",
    "        self.hparams = hparams\n",
    "\n",
    "        mobile_network = models.mobilenet_v2(pretrained=True)\n",
    "\n",
    "        layers = list(mobile_network.children())[:-1]  # 1x1280x8x8\n",
    "        layers.append(nn.Conv2d(1280, 120, 1, 1))  # 1x160x8x8\n",
    "        layers.append(nn.LeakyReLU(0.1))\n",
    "        layers.append(nn.Upsample(scale_factor=4))  # 1x160x32x32\n",
    "        layers.append(nn.ConvTranspose2d(120, 80, 3, 2))  # 1x120x64x64\n",
    "        layers.append(nn.LeakyReLU(0.1))\n",
    "        layers.append(nn.ConvTranspose2d(80, 60, 9, dilation=2))  # 1x80x80x80\n",
    "        layers.append(nn.LeakyReLU(0.1))\n",
    "        layers.append(nn.ConvTranspose2d(60, 40, 9, dilation=2))  # 1x60x96x96\n",
    "        layers.append(nn.LeakyReLU(0.1))\n",
    "        layers.append(nn.ConvTranspose2d(40, 40, 11, dilation=2))  # 1x40x116x116\n",
    "        layers.append(nn.LeakyReLU(0.1))\n",
    "        layers.append(nn.Upsample(scale_factor=2))  # 1x40x232x232\n",
    "        layers.append(nn.ConvTranspose2d(40, 23, 7))  # 1x23x240x240\n",
    "        layers.append(nn.LeakyReLU(0.1))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.network(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    \"lr\" : 0.001,\n",
    "    \"batch_size\" : 4,\n",
    "    \"num_epochs\" : 4\n",
    "}  \n",
    "\n",
    "model = SegmentationNN(hparams=hparams)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=hparams[\"lr\"])\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=-1, reduction='mean')\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=hparams[\"batch_size\"], shuffle=True)\n",
    "print(train_loader)\n",
    "print(next(iter(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (inputs, targets) in train_data[0:4]:\n",
    "    inputs, targets = inputs, targets\n",
    "    outputs = model(inputs.unsqueeze(0).to(device))\n",
    "    losses = criterion(outputs, targets.unsqueeze(0).to(device))\n",
    "    print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('training starts!')\n",
    "for epoch in range(hparams[\"num_epochs\"]):\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(train_loader):\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(images)\n",
    "        loss = criterion(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    print(\"Epoch: %d Loss: %.3f\" % (epoch + 1, epoch_loss / 276))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
