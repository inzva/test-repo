Week4
Pytorch, EDA, Traditional ML Algorithms: Linear-Logistic Regression, KNN, KMeans, SVM, Naive Bayes, Decision Trees, Random Trees, PCA, LDA



Theoreticals: Week 4 Theory, Week 3 Revisited, Week 2&1 Reminded	11.05-12.15 	(check appendix week4)


break									12.15-12.25



Pytorch Image Classification						12.25-13.15
lunch									13.15-13.45


Wine Customer Segment Classification					13.45-14.30
break									14.30-14.40


Heart Disease Classification						14.40-15.40
break									15.40-15.50


Credit Card Fraud Detection						15.50-16.30

Sites									16.30-17.00
	-



Course Summary. Learning Topics Rementioned				15 mins


timeline buffer:							15 mins
estimated finish:							17.30





homework4 assignment: 	Gradient Boosting for Classification		17.00-17.15
	https://www.kaggle.com/kabure/german-credit-data-with-risk

	















Appendix Week 4:
	Theoreticals:

		Week 1 Topics Reminded:
			-dense and conv network types, frameworks and tools, gans, structuring approaches to problems
		Week 2 Reminded:
			- sequential models, lstms, embeddings, spam detection, nlp terms, oil prices
			- seq_len, delta, batch_size, window_stride_to_create_examples, inp out intersection

		Week 3 Revisited:
			- Autoencoders, PCA, LDA. 
			- Reconstruction loss, Anomaly detection, Real Time Data Acquisition, Iot, Cloud Computing, Edge Computing
			- Arduino, Ibm Watson Iot, Node Red
			- Distributed computing. Frameworks, usages.
			- Light AI, embedding models to chips, compressing models, models learning from other models
			- Char-Based Generators
				
				
		Week 4 Theory:

			- Linear Regression				
			- Logistic Regression
			- K-nearest neighbour
			- K-means clustering
				- check: https://www.youtube.com/watch?v=yR7k19YBqiw
				- elbow point
			- Support Vector Machines
				kernel trick
			- Bayesian Methods: Naive Bayes
				- marginal, joint, conditional probability
				- sum rule, product rule, bayes rule
				- bayesian inference
				- why naive bayes is naive
			- Decision Trees
			- Ensembles, Bootstrap Aggregation (Bagging): Random Forest
			- Stagewise Additive Modelling / Boosting: GBM, AdaBoost
				- (serial) boosting on residuals
			- PCA, LDA
				use cases: compression, visualization
				
			- Error Metrics: Accuracy, Precision, Recall. Confusion Matrix

			reference: https://www.coursera.org/learn/advanced-machine-learning-signal-processing/







		
		Other topics for interested ones:


			Sub-Expertise Areas in Machine Learning:

				- Attacking Machine Learning with Adverserial Examples
				- Interpretability In Deep Learning
				- Robust Deep Learning
				- Algorithmic Bias in Deep Learning: Discrimination by AI



			Other Algorithms:

				- Capsule Networks
				- Non-Negative Matrix Factorization for Recommender Systems
				- Reinforcement Learning
					With Criterion of Optimality
						Policy Learning
						State Value Learning

					With a Value Function
						Monte Carlo Methods
						Temporal Difference Learning
							Q-Learning

				- Evolutionary Algoritms
					Genetic Algorithms


				- Bayesian Inference
					bayesian algorithms: expectetion maximizing
					maximum a posteriori estimation

				- Manifold Alignment

				- Symbolic AI

				- Rule Based ML

				- Markov Chains
					 Hidden Markov Model

				- Artificial Immune System
