# Applied-AI-Study-Group

This is the repository for the content of inzva 2020-February Applied AI Study Group, guided by Ahmet Melek.

In the group we have worked on these subjects:

* Frameworks: Tensorflow, Keras, SystemML, DL4J, Apache Spark, Pytorch

* Topics: Image Classification, Image Generation (Image Restoration/Inpainting), Computer Vision (Detections), Text Classification, Time Series Forecasting, Anomaly Detection, Scaling Machine Learning Projects (Big Data Frameworks - Distributed Computing Frameworks)  Text Generation, PCA (Eigenface method), Structured Data Classification, Exploratory Data Analysis, Data Preprocessing, Data Acquisition, GANs (Generative Adverserial Networks)

* Architectures - Methods: Multilayer Perceptron (Fully-Connected Neural Networks), Convolutional Neural Networks (CNN), Long-Short Term Memory (LSTM), Autoencoders, GANs, Logistic Regression, KNN, SVM, Bayesian Methods, Decision Trees, PCA, LDA, Bootstrap Aggregating Methods, Gradient Boosting Methods

* Environments: Google Colab, IBM Watson Studio, Jupyter Notebook (Local)

## Weekly Summaries

### Week1

We have worked on four problems:

* Image Classification with MNIST dataset on tensorflow, using Fully-Connected Neural Networks.

* Image Classification with MNIST dataset on keras functional API, using Convolutional Neural Networks.

* Image Classification with CIFAR-10 dataset on keras sequential API, using Convolutional Neural Networks.

* Image Generation with aligned UTK dataset on keras, using Deep Convolutional Generative Adverserial Networks (DCGAN Autoencoder). 

For all examples in Week1, we have worked on Google Colab.

### Week2

We have worked on three problems:

* NLP: Word Embeddings and latent space operations with a custom mini-dataset on keras, using Fully-Connected Neural Networks.

* NLP: Text classification with SMS Spam Collection Dataset on keras, using Fully-Connected Neural Networks. 

* Sequential data preparation and timeseries forecasting with Federal Reserve Economic Data Crude Oil Prices Chart on keras, using LSTM Networks. 



For all examples in Week2, we have worked on Google Colab.

### Week3

We have worked on five problems:

* Sequential data preparation and anomaly detection with Bearing Data Center Seeded Fault Test dataset on keras, using LSTM Autoencoders.

* Solution of Homework-2 was shown. Data acquisition, gentle scraping. Concept of temperature on generative models.

* Converting Keras models to DL4J models. Converting DL4J models to Apache Spark models via SystemML. After that, making classification with Iris dataset on Apache Spark using Fully-Connected Neural Networks. Worked on IBM watson studio.

* Converting Keras models directly to Apache Spark models via SystemML. After that, making anomaly detection using Bearing Data Center Seeded Fault Test dataset on keras, using LSTM Autoencoders.

* Briefly mentioned on the Eigenface method which is a traditional machine learning method, and used it to make biometric identification and face reconstruction from face encodings, on ORL dataset.

Also mentioned on real time data acquisition with IBM Node Red, cloud computing, edge computing, distributed computing and making deep learning models work faster.

### Week4

We have worked on three problems:

* Image Classification with CIFAR-10 dataset on Pytorch, using Convolutional Neural Networks.

* Exploratory data analysis such as plotting, drawing charts, analyzing on Kaggle Wine Dataset, ULB Credit Card Fraud Detection Dataset and UCI Heart Disease Dataset.

* Structured Data Classification with the above-mentioned datasets, using traditional machine learning methods such as Logistic Regression, KNN, SVM, Bayesian Methods, Decision Trees, PCA, LDA, Bootstrap Aggregating Methods and Gradient Boosting Methods.


### Homework 1

In homework one, participants take an aligned hand image dataset with keypoint labels, and try to preprocess the dataset and make regression on the keypoint coordinates.


### Homework 2

In homework two, participants scrape newspaper data from the internet and use a prepared model to generate text on the collected data.


