{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# both keras and tf datasets can be used. TFDS will be used to show the list of datasets.\n",
    "# TFDS is a high-level wrapper around tf.data.\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abstract_reasoning',\n",
       " 'accentdb',\n",
       " 'aeslc',\n",
       " 'aflw2k3d',\n",
       " 'ag_news_subset',\n",
       " 'ai2_arc',\n",
       " 'ai2_arc_with_ir',\n",
       " 'amazon_us_reviews',\n",
       " 'anli',\n",
       " 'arc',\n",
       " 'bair_robot_pushing_small',\n",
       " 'bccd',\n",
       " 'beans',\n",
       " 'big_patent',\n",
       " 'bigearthnet',\n",
       " 'billsum',\n",
       " 'binarized_mnist',\n",
       " 'binary_alpha_digits',\n",
       " 'blimp',\n",
       " 'bool_q',\n",
       " 'c4',\n",
       " 'caltech101',\n",
       " 'caltech_birds2010',\n",
       " 'caltech_birds2011',\n",
       " 'cars196',\n",
       " 'cassava',\n",
       " 'cats_vs_dogs',\n",
       " 'celeb_a',\n",
       " 'celeb_a_hq',\n",
       " 'cfq',\n",
       " 'chexpert',\n",
       " 'cifar10',\n",
       " 'cifar100',\n",
       " 'cifar10_1',\n",
       " 'cifar10_corrupted',\n",
       " 'citrus_leaves',\n",
       " 'cityscapes',\n",
       " 'civil_comments',\n",
       " 'clevr',\n",
       " 'clic',\n",
       " 'clinc_oos',\n",
       " 'cmaterdb',\n",
       " 'cnn_dailymail',\n",
       " 'coco',\n",
       " 'coco_captions',\n",
       " 'coil100',\n",
       " 'colorectal_histology',\n",
       " 'colorectal_histology_large',\n",
       " 'common_voice',\n",
       " 'coqa',\n",
       " 'cos_e',\n",
       " 'cosmos_qa',\n",
       " 'covid19sum',\n",
       " 'crema_d',\n",
       " 'curated_breast_imaging_ddsm',\n",
       " 'cycle_gan',\n",
       " 'deep_weeds',\n",
       " 'definite_pronoun_resolution',\n",
       " 'dementiabank',\n",
       " 'diabetic_retinopathy_detection',\n",
       " 'div2k',\n",
       " 'dmlab',\n",
       " 'downsampled_imagenet',\n",
       " 'dsprites',\n",
       " 'dtd',\n",
       " 'duke_ultrasound',\n",
       " 'e2e_cleaned',\n",
       " 'emnist',\n",
       " 'eraser_multi_rc',\n",
       " 'esnli',\n",
       " 'eurosat',\n",
       " 'fashion_mnist',\n",
       " 'flic',\n",
       " 'flores',\n",
       " 'food101',\n",
       " 'forest_fires',\n",
       " 'fuss',\n",
       " 'gap',\n",
       " 'geirhos_conflict_stimuli',\n",
       " 'genomics_ood',\n",
       " 'german_credit_numeric',\n",
       " 'gigaword',\n",
       " 'glue',\n",
       " 'goemotions',\n",
       " 'gpt3',\n",
       " 'groove',\n",
       " 'gtzan',\n",
       " 'gtzan_music_speech',\n",
       " 'hellaswag',\n",
       " 'higgs',\n",
       " 'horses_or_humans',\n",
       " 'i_naturalist2017',\n",
       " 'imagenet2012',\n",
       " 'imagenet2012_corrupted',\n",
       " 'imagenet2012_real',\n",
       " 'imagenet2012_subset',\n",
       " 'imagenet_a',\n",
       " 'imagenet_r',\n",
       " 'imagenet_resized',\n",
       " 'imagenet_v2',\n",
       " 'imagenette',\n",
       " 'imagewang',\n",
       " 'imdb_reviews',\n",
       " 'irc_disentanglement',\n",
       " 'iris',\n",
       " 'kitti',\n",
       " 'kmnist',\n",
       " 'lambada',\n",
       " 'lfw',\n",
       " 'librispeech',\n",
       " 'librispeech_lm',\n",
       " 'libritts',\n",
       " 'ljspeech',\n",
       " 'lm1b',\n",
       " 'lost_and_found',\n",
       " 'lsun',\n",
       " 'malaria',\n",
       " 'math_dataset',\n",
       " 'mctaco',\n",
       " 'mlqa',\n",
       " 'mnist',\n",
       " 'mnist_corrupted',\n",
       " 'movie_lens',\n",
       " 'movie_rationales',\n",
       " 'movielens',\n",
       " 'moving_mnist',\n",
       " 'multi_news',\n",
       " 'multi_nli',\n",
       " 'multi_nli_mismatch',\n",
       " 'natural_questions',\n",
       " 'natural_questions_open',\n",
       " 'newsroom',\n",
       " 'nsynth',\n",
       " 'nyu_depth_v2',\n",
       " 'omniglot',\n",
       " 'open_images_challenge2019_detection',\n",
       " 'open_images_v4',\n",
       " 'openbookqa',\n",
       " 'opinion_abstracts',\n",
       " 'opinosis',\n",
       " 'opus',\n",
       " 'oxford_flowers102',\n",
       " 'oxford_iiit_pet',\n",
       " 'para_crawl',\n",
       " 'patch_camelyon',\n",
       " 'paws_wiki',\n",
       " 'paws_x_wiki',\n",
       " 'pet_finder',\n",
       " 'pg19',\n",
       " 'piqa',\n",
       " 'places365_small',\n",
       " 'plant_leaves',\n",
       " 'plant_village',\n",
       " 'plantae_k',\n",
       " 'qa4mre',\n",
       " 'qasc',\n",
       " 'quac',\n",
       " 'quickdraw_bitmap',\n",
       " 'radon',\n",
       " 'reddit',\n",
       " 'reddit_disentanglement',\n",
       " 'reddit_tifu',\n",
       " 'resisc45',\n",
       " 'robonet',\n",
       " 'rock_paper_scissors',\n",
       " 'rock_you',\n",
       " 'salient_span_wikipedia',\n",
       " 'samsum',\n",
       " 'savee',\n",
       " 'scan',\n",
       " 'scene_parse150',\n",
       " 'scicite',\n",
       " 'scientific_papers',\n",
       " 'sentiment140',\n",
       " 'shapes3d',\n",
       " 'smallnorb',\n",
       " 'snli',\n",
       " 'so2sat',\n",
       " 'speech_commands',\n",
       " 'spoken_digit',\n",
       " 'squad',\n",
       " 'stanford_dogs',\n",
       " 'stanford_online_products',\n",
       " 'starcraft_video',\n",
       " 'stl10',\n",
       " 'story_cloze',\n",
       " 'sun397',\n",
       " 'super_glue',\n",
       " 'svhn_cropped',\n",
       " 'ted_hrlr_translate',\n",
       " 'ted_multi_translate',\n",
       " 'tedlium',\n",
       " 'tf_flowers',\n",
       " 'the300w_lp',\n",
       " 'tiny_shakespeare',\n",
       " 'titanic',\n",
       " 'trec',\n",
       " 'trivia_qa',\n",
       " 'tydi_qa',\n",
       " 'uc_merced',\n",
       " 'ucf101',\n",
       " 'vctk',\n",
       " 'vgg_face2',\n",
       " 'visual_domain_decathlon',\n",
       " 'voc',\n",
       " 'voxceleb',\n",
       " 'voxforge',\n",
       " 'waymo_open_dataset',\n",
       " 'web_questions',\n",
       " 'wider_face',\n",
       " 'wiki40b',\n",
       " 'wiki_bio',\n",
       " 'wikihow',\n",
       " 'wikipedia',\n",
       " 'wikipedia_toxicity_subtypes',\n",
       " 'wine_quality',\n",
       " 'winogrande',\n",
       " 'wmt14_translate',\n",
       " 'wmt15_translate',\n",
       " 'wmt16_translate',\n",
       " 'wmt17_translate',\n",
       " 'wmt18_translate',\n",
       " 'wmt19_translate',\n",
       " 'wmt_t2t_translate',\n",
       " 'wmt_translate',\n",
       " 'wordnet',\n",
       " 'xnli',\n",
       " 'xquad',\n",
       " 'xsum',\n",
       " 'yelp_polarity_reviews',\n",
       " 'yes_no']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of datasets\n",
    "tfds.list_builders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary libraries for data exploration and further data operations\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use tfds api to get mnist dataset\n",
    "# split to train and test\n",
    "# batch size -1, thus, no batch but could be done here.\n",
    "mnist_training, mnist_test = tfds.load('mnist', split=['train', 'test'], batch_size=-1, as_supervised=True)                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(60000, 28, 28, 1), dtype=uint8, numpy=\n",
      "array([[[[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]]],\n",
      "\n",
      "\n",
      "       [[[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]]],\n",
      "\n",
      "\n",
      "       [[[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]]],\n",
      "\n",
      "\n",
      "       [[[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]]],\n",
      "\n",
      "\n",
      "       [[[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]]]], dtype=uint8)>, <tf.Tensor: shape=(60000,), dtype=int64, numpy=array([4, 1, 0, ..., 6, 1, 5])>)\n"
     ]
    }
   ],
   "source": [
    "print(mnist_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(10000, 28, 28, 1), dtype=uint8, numpy=\n",
      "array([[[[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]]],\n",
      "\n",
      "\n",
      "       [[[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]]],\n",
      "\n",
      "\n",
      "       [[[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]]],\n",
      "\n",
      "\n",
      "       [[[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]]],\n",
      "\n",
      "\n",
      "       [[[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]]]], dtype=uint8)>, <tf.Tensor: shape=(10000,), dtype=int64, numpy=array([2, 0, 4, ..., 8, 0, 5])>)\n"
     ]
    }
   ],
   "source": [
    "print(mnist_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_training_images, mnist_training_labels = mnist_training[0], mnist_training[1]\n",
    "mnist_test_images, mnist_test_labels = mnist_test[0], mnist_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(60000,)\n",
      "(10000, 28, 28, 1)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(mnist_training_images.shape)\n",
    "print(mnist_training_labels.shape)\n",
    "\n",
    "print(mnist_test_images.shape)\n",
    "print(mnist_test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(4, shape=(), dtype=int64)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's visualize first training image\n",
    "plt.imshow(mnist_training_images[0] ,cmap = 'gray')\n",
    "print(mnist_training_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(2, shape=(), dtype=int64)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAO00lEQVR4nO3df4xV9ZnH8c+jWzWRKoy4ZEJdacE/xEnWbghusjiywTbuGAUS0xTRqEXHxJJU3bhLMApRq2bX7v5hDAlYKW5asEYqpq4BF4hjjTaMxlXApQpBy28VDfYPwIFn/5iDGXHO9wzn/jh35nm/ksnce5577nm8zodz7v3ec77m7gIw8p1WdQMAmoOwA0EQdiAIwg4EQdiBIP6qmRszMz76BxrM3W2w5TXt2c3sKjPbZmYfmNmCWp4LQGNZ2XF2Mztd0p8k/UDSLkmbJM1x962JddizAw3WiD37VEkfuPsOdz8qaZWkmTU8H4AGqiXs4yX9ecD9XdmyrzGzbjPrNbPeGrYFoEYN/4DO3ZdKWipxGA9UqZY9+25JFwy4/51sGYAWVEvYN0m6yMy+a2ZnSPqxpBfq0xaAeit9GO/ufWY2X9JaSadLesrdt9StMwB1VXrordTGeM8ONFxDvlQDYPgg7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiCIpk7ZDJyK9vb2ZL2trS1Z7+vry61t27atVE/DGXt2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCcXZUZtKkScn6xo0bk/Wicfgvv/wyt7ZkyZLkunfffXeyPhzVFHYz2ynpC0nHJPW5+5R6NAWg/uqxZ/9Hd/+kDs8DoIF4zw4EUWvYXdI6M3vTzLoHe4CZdZtZr5n11rgtADWo9TB+mrvvNrO/lvSymf2fu/cMfIC7L5W0VJLMzGvcHoCSatqzu/vu7PcBSb+TNLUeTQGov9JhN7OzzezbJ25L+qGkzfVqDEB9mXu5I2sz+5769+ZS/9uB37j7zwvW4TC+yTo7O5P1Z599Nlkv+vtYvnx56e13dHQk1x01alSyXvZvV0qPwUvSa6+9lqxfeeWVpbfdaO5ugy0v/Z7d3XdI+tvSHQFoKobegCAIOxAEYQeCIOxAEIQdCIJTXEeA0aNH59aKhsbGjh2brBcNb91zzz3JesqePXuS9Xnz5pV+bklatGhRbu3iiy9Ornv06NGatt2K2LMDQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCMsw8DU6emrwny0EMP5dYuvPDCerfzNUXj+Dt27Ci97r59+0r1dMKDDz5Yet3t27fXtO1WxJ4dCIKwA0EQdiAIwg4EQdiBIAg7EARhB4JgnH0Y6OrqStZnzJhR+rmLLpk8Z86cZH337t2lt91obW1tuTWzQa+2/JWDBw/Wu53KsWcHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAYZx8GtmzZkqynpl3evHlzct3UufCt7tZbb03WzznnnNxa0fXwn3nmmVI9tbLCPbuZPWVmB8xs84BlbWb2spm9n/0e09g2AdRqKIfxv5J01UnLFkha7+4XSVqf3QfQwgrD7u49kk7+7uBMSSuy2yskzapvWwDqrex79nHuvje7vU/SuLwHmlm3pO6S2wFQJzV/QOfubma5n3a4+1JJSyUp9TgAjVV26G2/mbVLUvb7QP1aAtAIZcP+gqSbsts3SVpTn3YANIoVjTea2UpJ0yWNlbRf0iJJz0v6raS/kfShpB+5e+EJwBzGo542bNiQrHd2dubW1q9fn1z36quvTtb7+vqS9Sq5+6An6xe+Z3f3vKsXlL9iAoCm4+uyQBCEHQiCsANBEHYgCMIOBMEprmhZl112WbI+efLk0s+9bNmyZL2Vh9bKYs8OBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0Ewzo7KdHR0JOsvvvhisj569OhkvaenJ7e2bt265LojEXt2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCcfYmuOSSS5L1WbNmJevXXnttsj5lypRTbekrp52W/vf++PHjyfqmTZtK1+fMybtwcb/zzjsvWf/888+T9cWLF+fWDh06lFx3JGLPDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBFE7ZXNeNDeMpm6+77rrc2h133JFc94orrkjWm/n/4GRmg87u+5VW7u2GG25I1leuXFnPdoaNvCmbC/fsZvaUmR0ws80Dli02s91m9nb201XPZgHU31AO438l6apBlv+nu1+a/fx3fdsCUG+FYXf3HkkHm9ALgAaq5QO6+Wb2TnaYPybvQWbWbWa9ZtZbw7YA1Khs2JdImijpUkl7Jf0i74HuvtTdp7h7+bM1ANSsVNjdfb+7H3P345KWSZpa37YA1FupsJtZ+4C7syVtznssgNZQeD67ma2UNF3SWDPbJWmRpOlmdqkkl7RT0u2Na7E5Zs+enaw//fTTubUzzjgjue7HH3+crBeNZS9fvjxZP3z4cG5t1apVyXU/++yzZP2BBx5I1m+77bZkvZH27NlT2baHo8Kwu/tgVxj4ZQN6AdBAfF0WCIKwA0EQdiAIwg4EQdiBIMJcSjp1iqqUHlqT0sNrRUNjVQ5PFbn//vuT9aIhySrNnTs3WX/99ddza0ePHq13Oy2PPTsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBBHmUtIbNmxI1js7O5P11Fj6/Pnzk+seOXIkWa/V+PHjc2v33ntvct3bb0+fnVz091E0ZfPDDz+cW7vllluS686cOTNZL+rtrrvuyq09/vjjyXWHs9KXkgYwMhB2IAjCDgRB2IEgCDsQBGEHgiDsQBAjZpx92rRpyforr7ySrG/bti1Znzx58in3NFQTJkxI1qdPn56sL1y4MLc2ceLE5LpF53U/9thjyfqaNWuS9d7e8rN+ffrpp8n66NGjk/Wenp7cWtEY/qFDh5L1VsY4OxAcYQeCIOxAEIQdCIKwA0EQdiAIwg4EMWKuG1903nbR9wmKpjZOmTRpUrI+Y8aMZD11zrcknXvuuafc0wlr165N1ouuG1/LOHmturq6kvXnn38+Wb/88stza0888URy3RtvvDFZH44K9+xmdoGZbTSzrWa2xcx+li1vM7OXzez97PeYxrcLoKyhHMb3Sfpnd58s6e8l/dTMJktaIGm9u18kaX12H0CLKgy7u+9197ey219Iek/SeEkzJa3IHrZC0qwG9QigDk7pPbuZTZD0fUl/lDTO3fdmpX2SxuWs0y2pu4YeAdTBkD+NN7NRkp6TdKe7f+0sAe//9GvQT8Dcfam7T3H3KTV1CqAmQwq7mX1L/UH/tbuvzhbvN7P2rN4u6UBjWgRQD4WnuJqZqf89+UF3v3PA8n+X9Km7P2pmCyS1ufu/FDxXw05xPXbsWLJe9N9ZdArsWWedlVvr6OhIrjtq1Khk/fDhw8n6/v37k/Xrr78+t1Y0dNbX15est7LVq1cn69dcc01u7aOPPkquW3R58JdeeilZr1LeKa5Dec/+D5JulPSumb2dLVso6VFJvzWzeZI+lPSjOvQJoEEKw+7uf5A06L8UktLfFgHQMvi6LBAEYQeCIOxAEIQdCIKwA0GMmEtJP/nkk8n6zTffXNPzb926Nbe2cePG5Lqvvvpqsr5r165k/Y033kjWMbgVK1bk1ubOnZtc97777kvWH3nkkVI9NQOXkgaCI+xAEIQdCIKwA0EQdiAIwg4EQdiBIEbMOPuZZ56ZrBdNXVwkNRY+nKf3HcnOP//8UjVJ2r59e7J+5MiRUj01A+PsQHCEHQiCsANBEHYgCMIOBEHYgSAIOxDEiBlnB9CPcXYgOMIOBEHYgSAIOxAEYQeCIOxAEIQdCKIw7GZ2gZltNLOtZrbFzH6WLV9sZrvN7O3sp6vx7QIoq/BLNWbWLqnd3d8ys29LelPSLPXPx/4Xd39syBvjSzVAw+V9qWYo87PvlbQ3u/2Fmb0naXx92wPQaKf0nt3MJkj6vqQ/Zovmm9k7ZvaUmY3JWafbzHrNrLe2VgHUYsjfjTezUZJekfRzd19tZuMkfSLJJT2o/kP9nxQ8B4fxQIPlHcYPKexm9i1Jv5e01t3/Y5D6BEm/d/eOguch7ECDlT4RxsxM0i8lvTcw6NkHdyfMlrS51iYBNM5QPo2fJulVSe9KOp4tXihpjqRL1X8Yv1PS7dmHeannYs8ONFhNh/H1QtiBxuN8diA4wg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCFF5yss08kfTjg/thsWStq1d5atS+J3sqqZ28X5hWaej77NzZu1uvuUyprIKFVe2vVviR6K6tZvXEYDwRB2IEgqg770oq3n9KqvbVqXxK9ldWU3ip9zw6geareswNoEsIOBFFJ2M3sKjPbZmYfmNmCKnrIY2Y7zezdbBrqSueny+bQO2BmmwcsazOzl83s/ez3oHPsVdRbS0zjnZhmvNLXrurpz5v+nt3MTpf0J0k/kLRL0iZJc9x9a1MbyWFmOyVNcffKv4BhZp2S/iLp6RNTa5nZv0k66O6PZv9QjnH3f22R3hbrFKfxblBvedOM36wKX7t6Tn9eRhV79qmSPnD3He5+VNIqSTMr6KPluXuPpIMnLZ4paUV2e4X6/1iaLqe3luDue939rez2F5JOTDNe6WuX6Kspqgj7eEl/HnB/l1prvneXtM7M3jSz7qqbGcS4AdNs7ZM0rspmBlE4jXcznTTNeMu8dmWmP68VH9B90zR3/ztJ/yTpp9nhakvy/vdgrTR2ukTSRPXPAbhX0i+qbCabZvw5SXe6+6GBtSpfu0H6asrrVkXYd0u6YMD972TLWoK7785+H5D0O/W/7Wgl+0/MoJv9PlBxP19x9/3ufszdj0tapgpfu2ya8eck/drdV2eLK3/tBuurWa9bFWHfJOkiM/uumZ0h6ceSXqigj28ws7OzD05kZmdL+qFabyrqFyTdlN2+SdKaCnv5mlaZxjtvmnFV/NpVPv25uzf9R1KX+j+R3y7p3ip6yOnre5L+N/vZUnVvklaq/7DuS/V/tjFP0nmS1kt6X9L/SGprod7+S/1Te7+j/mC1V9TbNPUfor8j6e3sp6vq1y7RV1NeN74uCwTBB3RAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EMT/A5hYyA25NBcWAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# also, let's visualize first test image\n",
    "plt.imshow(mnist_test_images[0] ,cmap = 'gray')\n",
    "print(mnist_test_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n",
      "255 0\n",
      "255 0\n",
      "9 0\n",
      "9 0\n"
     ]
    }
   ],
   "source": [
    "# data preprocessing\n",
    "# reshape into trainable vectors\n",
    "num_training_images = mnist_training_images.shape[0]\n",
    "num_test_images = mnist_test_images.shape[0]\n",
    "\n",
    "img_width, img_height = mnist_training_images.shape[1], mnist_training_images.shape[2]\n",
    "\n",
    "# since dense layer, we have to flatten 28x28 to 784x1.\n",
    "mnist_training_images = tf.reshape(mnist_training_images, shape=(num_training_images, img_width * img_height))\n",
    "mnist_test_images = tf.reshape(mnist_test_images, shape=(num_test_images, img_width * img_height))\n",
    "\n",
    "# check the changes\n",
    "print(mnist_training_images.shape)\n",
    "print(mnist_test_images.shape)\n",
    "\n",
    "# another preprocessing step is to normalize data\n",
    "print(np.amax(mnist_training_images[0]),np.amin(mnist_training_images[0]))\n",
    "\n",
    "print(np.amax(mnist_test_images[0]),np.amin(mnist_test_images[0]))\n",
    "\n",
    "print(np.amax(mnist_training_labels),np.amin(mnist_training_labels))\n",
    "\n",
    "print(np.amax(mnist_test_labels),np.amin(mnist_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversion of data type and normalization of training data\n",
    "# main idea of normalization/standardization -> variables that are at different scale contribute different.\n",
    "# we want to reduce the \"bias\" as much as possible by these methods.\n",
    "# min-max is highly influenced by outliers! min and max values affect a lot!\n",
    "def preprocess(x, y):\n",
    "  x = tf.cast(x, tf.float32) / 255.0\n",
    "  y = tf.cast(y, tf.int64)\n",
    "\n",
    "  return x, y\n",
    "\n",
    "# one-hot labels and create dataloader with given batch size.\n",
    "def create_dataset(xs, ys, n_classes=10):\n",
    "  ys = tf.one_hot(ys, depth=n_classes)\n",
    "  return tf.data.Dataset.from_tensor_slices((xs, ys)) \\\n",
    "    .map(preprocess) \\\n",
    "    .shuffle(len(ys)) \\\n",
    "    .batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = create_dataset(mnist_training_images, mnist_training_labels)\n",
    "test_dataset = create_dataset(mnist_test_images, mnist_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((None, 784), (None, 10)), types: (tf.float32, tf.int64)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(None, 784), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(None, 10), dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_dataset)\n",
    "train_dataset.element_spec    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 784)\n",
      "(128, 10)\n",
      "tf.Tensor(\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.11372549 0.44705883 0.6666667  0.6666667  0.22352941 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.5529412  1.         1.\n",
      " 1.         1.         1.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.44705883\n",
      " 0.7764706  1.         0.8862745  0.3372549  0.         0.44705883\n",
      " 1.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.3372549  0.7764706  1.         1.         0.5529412\n",
      " 0.         0.         0.         0.44705883 0.8862745  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.3372549  0.44705883 1.\n",
      " 1.         1.         0.5529412  0.         0.         0.\n",
      " 0.         1.         0.3372549  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.44705883 1.         1.         1.         0.8862745  1.\n",
      " 0.11372549 0.         0.         0.         0.44705883 0.8862745\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.44705883 1.         1.\n",
      " 0.8862745  0.22352941 0.3372549  0.44705883 0.         0.\n",
      " 0.         0.11372549 1.         0.3372549  0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.7764706  1.         1.         0.5529412  0.         0.\n",
      " 0.         0.         0.         0.         0.         0.7764706\n",
      " 0.8862745  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.6666667  1.         1.\n",
      " 0.3372549  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.11372549 1.         0.44705883 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.6666667  1.         1.         1.         0.11372549 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.6666667  1.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.44705883 1.         0.8862745\n",
      " 0.7764706  1.         0.8862745  0.22352941 0.         0.\n",
      " 0.         0.         0.         0.11372549 1.         0.3372549\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.11372549 1.         0.8862745  0.         0.         0.5529412\n",
      " 0.7764706  0.7764706  0.         0.         0.         0.\n",
      " 0.         0.7764706  0.8862745  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.6666667  1.\n",
      " 0.11372549 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.3372549  1.\n",
      " 0.11372549 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.11372549 1.         0.3372549  0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.22352941 1.         0.6666667  0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.44705883\n",
      " 1.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.11372549 1.\n",
      " 0.8862745  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         1.         0.6666667  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.44705883 1.         0.8862745  0.11372549 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         1.         0.7764706  0.         0.         0.\n",
      " 0.         0.         0.11372549 0.44705883 0.8862745  1.\n",
      " 0.8862745  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         1.\n",
      " 1.         0.7764706  0.3372549  0.3372549  0.44705883 0.6666667\n",
      " 1.         1.         1.         0.5529412  0.11372549 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.3372549  1.         1.\n",
      " 1.         1.         1.         1.         0.8862745  0.5529412\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.3372549  0.6666667  0.6666667\n",
      " 0.44705883 0.22352941 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ], shape=(784,), dtype=float32)\n",
      "tf.Tensor([1 0 0 0 0 0 0 0 0 0], shape=(10,), dtype=int64)\n",
      "1.0 0.0\n"
     ]
    }
   ],
   "source": [
    "# check the dataloader\n",
    "batch_images, batch_labels = next(iter(train_dataset))\n",
    "print(batch_images.shape)\n",
    "print(batch_labels.shape)\n",
    "print(batch_images[0])\n",
    "print(batch_labels[0])\n",
    "print(np.amax(batch_images[0]),np.amin(batch_images[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize first batch training image to show it is corresponds to same class with printed label.\n",
    "plt.imshow(batch_images[0] ,cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining hyperparameters\n",
    "input_shape = 784\n",
    "label_shape = 10\n",
    "\n",
    "lr = 0.003\n",
    "\n",
    "layer_neurons = [\n",
    "    [input_shape, 200],\n",
    "    [200, 80],\n",
    "    [80, label_shape],\n",
    "]\n",
    "\n",
    "bias_shapes = [200, 80, label_shape]\n",
    "# xaiver uniform initializer\n",
    "initializer = tf.initializers.glorot_uniform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a dense layer, also, you can use TF2 API or Keras!\n",
    "def dense_layer(inputs, weights, bias):\n",
    "    return tf.nn.sigmoid(tf.matmul(inputs, weights) + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for initialization of weights and biases\n",
    "def get_weight(shape, name):\n",
    "    return tf.Variable(initializer(shape), name=name, trainable=True, dtype=tf.float32)\n",
    "\n",
    "def get_bias(shape, name):\n",
    "    return tf.Variable(initializer([shape]), name=name, trainable=True, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define weights and bias lists to use in model\n",
    "weights = []\n",
    "bias = []\n",
    "i = 0\n",
    "for layer in layer_neurons:\n",
    "    weights.append(get_weight(layer, 'weight{}'.format(i)))\n",
    "    i+=1\n",
    "\n",
    "i = 0\n",
    "for layer in bias_shapes:\n",
    "    bias.append(get_bias(layer, 'bias{}'.format(i)))\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model with initialized weights and biases\n",
    "def model(input):\n",
    "    l1 = dense_layer(input, weights[0], bias[0])\n",
    "    l2 = dense_layer(l1, weights[1], bias[1])\n",
    "    l3 = dense_layer(l2, weights[2], bias[2])\n",
    "    #return tf.nn.softmax(l3)\n",
    "    return l3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define optimizer and loss function\n",
    "optimizer = tf.optimizers.Adam(lr)\n",
    "\n",
    "# it is with logits because we return the predictions without applying softmax!\n",
    "# applied directly to prediction probabilities.\n",
    "def loss(pred, target):\n",
    "    return tf.nn.softmax_cross_entropy_with_logits(target, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define our train_step here\n",
    "# tf.GradientTape is used for recording operations for automatic differentiation. backward pass!\n",
    "def train_step(model, inputs, outputs, epoch):\n",
    "    epoch_loss_avg = None\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        current_loss = loss(model(inputs), outputs)\n",
    "        grads = tape.gradient(current_loss, weights)\n",
    "        optimizer.apply_gradients(zip(grads, weights))\n",
    "    \n",
    "    epoch_loss_avg = tf.reduce_mean(current_loss)\n",
    "    \n",
    "    return epoch_loss_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- On epoch 0 ---\n",
      "| Loss:  1.63582146\n",
      "--- On epoch 1 ---\n",
      "| Loss:  1.51476598\n",
      "--- On epoch 2 ---\n",
      "| Loss:  1.49688733\n",
      "--- On epoch 3 ---\n",
      "| Loss:  1.4878068\n",
      "--- On epoch 4 ---\n",
      "| Loss:  1.4821254\n",
      "--- On epoch 5 ---\n",
      "| Loss:  1.47803211\n",
      "--- On epoch 6 ---\n",
      "| Loss:  1.47508657\n",
      "--- On epoch 7 ---\n",
      "| Loss:  1.47348416\n",
      "--- On epoch 8 ---\n",
      "| Loss:  1.47134519\n",
      "--- On epoch 9 ---\n",
      "| Loss:  1.46974969\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "# batch by batch for each epoch -> traverse over all training dataset.\n",
    "# total loss is divided by number of iterations to get average loss for each batch.\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    i = 0\n",
    "    for train_data in train_dataset:\n",
    "        batch_images, batch_labels = train_data\n",
    "        iter_loss = train_step(model, batch_images, batch_labels, epoch)\n",
    "        epoch_loss += iter_loss\n",
    "        i+=1\n",
    "    print(\"--- On epoch {} ---\".format(epoch))\n",
    "    tf.print(\"| Loss: \", epoch_loss/i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.978\n"
     ]
    }
   ],
   "source": [
    "acc = 0 \n",
    "# use trained model over test dataset and normalize with number of test samples\n",
    "# obtain accuracy!\n",
    "for test_data in test_dataset:\n",
    "    batch_images, batch_labels = test_data\n",
    "    predictions = model(batch_images)\n",
    "    predictions = tf.nn.softmax(predictions)\n",
    "    equality = tf.math.equal(np.argmax(predictions, axis=1), np.argmax(batch_labels, axis=1))\n",
    "    acc += np.sum(equality)\n",
    "acc /= 10000\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
