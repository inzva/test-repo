
# Homework of Week 2

In the lecture we have covered different Gradient Boosting trees and different hyperparameter optimization techniques. We are going dig deep into these concepts in this homework.

In this homework, you are asked to use the data in https://www.kaggle.com/c/ieee-fraud-detection/data in order to create XGBoost, CatBoost, and LightGBM (optional) models and explore their performances using different hyperparameter optimization techniques.

- Make sure you have access to the dataset. You can use kaggle to train your model.
- Train the algorithms using their default parameters and compare their performances.
- Apply hyperparameter optimization techniques i.e. Bayesian Optimization, Random Search to each algorithm.
- Compare performances after the hyperparameter optimization. Give the same hyperparameter search space in each technique.
- Which hyperparameter optimization technique performs the best and why? Is it the best because it is the fastest and the most accurate or are there any trade-off between these two? Explore and answer.

Do not forget to select the most appropriate model evaluation metrics for this problem.
